#+TITLE: How To Level Up
#+AUTHOR: Nathan Yong <u5355375@anu.edu.au>
#+CATEGORY: rainyday
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 11pt]
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setmainfont{Syntax LT Std}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{fullpage}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \allowdisplaybreaks[3]
#+LATEX_HEADER: \usepackage{makeidx}
#+LATEX_HEADER: \makeindex

#+BEGIN_LaTeX
\newcommand{\lra}{\leftrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\Ra}{\Rightarrow}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
#+END_LaTeX

Don't pay attention to the number values. They're liable to change at any time.

* Prelude

** Introduction
\label{intro}

Pierre Portal, my first year Calculus/Analysis lecturer, described understanding
mathematics in several levels, from zero to three.

- Level Zero: you fell asleep in the lecture, and know nothing.
- Level One: you know the formulas, and can apply them to examples.
- Level Two: you understand the formula, and have an intuition of why it
  works. You might be able to extend your knowledge of the formula to a related
  use case.
- Level Three: you understand the formula from a mathematical level, and you can
  prove it. You might be able to extend your knowledge of the formula to derive
  more complicated proofs and properties of mathematical objects you know
  about.

That's the title of this book explained. Introductory mathematics courses might
teach content at Level 1 or Level 2. High achieving high school
mathematics students might wish to understand content at Level 2. But in order
to get to serious concepts in maths, it becomes increasingly necessary to
understand at Level 3. This book is, then, an attempt to help you 'level up'.

This book probably won't help you. Pierre is probably totally correct, and you
need to really stare at the problems hard until you just get them. Nevertheless,
this book is an attempt at understanding proof.

** Sources
\label{sources}

For this book I have drawn on a variety of sources, including:

- Spivak's "Calculus"
- Trench's "Introduction to Real Analysis"
- Axler's "Linear Algebra Done Right"
- Lay's "Linear Algebra and its Applications"
- Beezer's "A First Course in Linear Algebra"
- Pierre Portal's own Calculus notes for 2013

* TODO Basics of Number Theory
\label{number-theory}

Before we can understand how functions of numbers work, we must first understand numbers.

* TODO Basics of Linear Algebra
\label{linear-algebra}
Because Linear Algebra really doesn't need several semesters. It's syntax.

** Vectors
\label{vectors}

On a real number line, we can describe any point by a single number. In more
than one dimension, to describe a point, we need more than a single number. In
2D space ($\mathbb{R}^2$), we conventionally denote points by their $x$ and $y$
coordinates.
\[ (x, y) \in \mathbb{R}^2
\]

In three dimensions, we'd have three coordinates, and in higher dimensional
space we'd need even more. (We actually mean a vector space: see
\nameref{vector-spaces}).

We can define vectors as a tuple (fixed collection) of numbers, or in other
forms. The following is /column vector/ \index{column vector} form.
\[ \begin{bmatrix}
x_1 \\ \vdots \\ x_n
\end{bmatrix} \in \mathbb{R}^n \]

There is also row vector form, which is much the same, except horizontally.

*** TODO Vector Operations
We can perform operations on these points, like adding them together, which
is pretty much what you expect.
\[ (x_1, y_1) + (x_2, y_2) = (x_1 + x_2, y_1 + y_2) \in \mathbb{R}^2
\]

Subtraction is quite similar.

There are a few slightly more interesting operations on vectors we can
define. For example, what is the product of two multidimensional numbers? Does
it make sense to multiply $(3, 9)$ by $2$? What about $(1, 2) \times (3, 4, 5)$?

The notion of a multi-dimensional product isn't immediately obvious, but we have
at our disposal one useful notion, the /vector dot product/, which is the
sum of the product of every component of a vector:
\[\begin{bmatrix} 1 \\ 3 \end{bmatrix} . \begin{bmatrix} 2 \\ 0 \end{bmatrix}
  = 1 \times 2 + 3 \times 0 = 2
\]

This takes two vectors in the same space and returns a real number. For
applications of the dot product, we have ??????????????????????????????????

** TODO Vector Spaces
\label{vector-spaces}

** TODO Matrices
\label{matrices}

* TODO Sequences and Series

To be continued...

* TODO Single-variable calculus in one dimension (The Basics)
What is calculus, and why do we study it?

The mathematical areas of study that most people mean when they say "calculus"
are the integral and differential calculus: the study of how functions change
and the areas of functions.

** TODO Continuity in one dimension
\label{continuity}

** TODO Limits in one dimension

** TODO Differential Calculus in one dimension

** TODO Integral Calculus in one dimension

** TODO Intermediate Value Theorem in one dimension
\label{ivt}

** TODO Mean Value Theorem in one dimension
\label{mvt}

** TODO Generalised Mean Value Theorem in one dimension
\label{gmvt}

* TODO Single-variable calculus in multiple dimensions ($\mathbb{R} \to \mathbb{R}^n$)
\label{one-to-many-calculus}

One-variable calculus in multiple dimensions is not particularly
terrifying. From Linear Algebra, we already know that a function from
$\mathbb{R} \to \mathbb{R}^n$ will have a one-dimensional column space and a
very large null space, so there is really only one dimension of information we
need to work with.

It may be beneficial to treat multi-dimensional real numbers as vectors, or
tuples of numbers.

** Norm and properties of numbers in $\mathbb{R}^n$
Requires: \nameref{number-theory}, \nameref{vectors}

We need a property of distance in multi-dimensional space, or a vector
space. The one we shall use for this section is the standard Euclidean norm,
which is a generalisation of the one-dimensional absolute value. We denote the
norm of a vector as such:
\[\forall (x_1, \dots, x_n) \in \mathbb{R}^n \quad
  \norm{(x_1, \dots, x_n)} := \left(\sum_{j=1}^n x_j^2\right)^{1/2}
\]

That is to say, the norm of a vector is defined as the square root of the sum
the squares of its individual components.

The important thing to note is that the norm has the property that whenever two
points are together in multi-dimensional space (their coordinates are close to
each other), then we have that the norm of their difference will be small.

** TODO Continuity ($\mathbb{R} \to \mathbb{R}^n$)
\label{one-to-many-continuity}

Recall \nameref{continuity}. A function $f: \mathbb{R} \to \mathbb{R}^n$ has
continuity defined similarly, since we only have to track one input variable. It
is continuous at point $t \in \mathbb{R}$ if:
\[ \forall \epsilon > 0,\ \exists \delta > 0,\ |h| < \delta \Ra \norm{f(t+h) -
f(t)} < \epsilon
\]

That is to say, for any error ($\epsilon$), there is a difference such that
whenever we shift the input of the function by an amount ($h$) smaller than that
distance ($\delta$), our function error will be smaller. We can always move
closer and closer to our point, with the error decreasing.

What does the second term mean, specifically?

** TODO Differential Calculus ($\mathbb{R} \to \mathbb{R}^n$)
\label{one-to-many-differential}

** TODO Integral Calculus ($\mathbb{R} \to \mathbb{R}^n$)
\label{one-to-many-integral}

* TODO Multi-variable calculus in one dimension
\label{many-to-one-calculus}

** TODO Arc Length and Curvature
\label{arc-length} \label{curvature}

** TODO Differentiability $\mathbb{R}^n \to \mathbb{R}$
Requires: linear maps, matrices

*** TODO Linearity of differentiability
Let $f, g: \mathbb{R}^n \to \mathbb{R}$ be differentiable at $x \in \mathbb{R}^n$. Let $\lambda \in
\mathbb{R}$. Then:

1. $f + \lambda g$ is differentiable at $x \in \mathbb{R}^n$ and $d_x(f + \lambda g) =
   d_x f + \lambda d_xg$
2. $f.g$ is differentiable at $x \in \mathbb{R}^n$ and $d_x(f.g) = f(x)d_xg + g(x)d_xf$

$f . g$ is function multiplication.

*PROOF* here goes helloooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

*** TODO Chain rule
Let $f: \mathbb{R}^n \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}^n$, and
we have $x$ such that $g$ is differentiable at $t \in \mathbb{R}$ and $f$ is
differentiable at the point $g(t) \in \mathbb{R}^n$.

Let $h: \mathbb{R} \to \mathbb{R}, t \mapsto f(g(t))$

Then $h$ if differentiable at $t \in \mathbb{R}$ and we have:
\begin{equation*}
h'(t) = d_{g(t)}f(g'(t))
\end{equation*}

*** TODO Differentiability, again
Requires: Mean Value Theorem, multivariable differentibility

Let $f: \mathbb{R}^n \to \mathbb{R}$ and $x \in \mathbb{R}^n$.

Assume that $f$ admits partial derivatives at $x \in \mathbb{R}^n$ and $\exists R > 0,
\forall j = 1 .. n, \partial_{x_j}f$ is continuous. in something something
something HELP

*** TODO Applications of Differentiation (Optimisation)
*Definition*: Let $f: \mathbb{R}^n \to \mathbb{R}$ and $x \in \mathbb{R}^n$.
- $x$ is a local minimum of $f$ if $\exists R > 0.\ \forall y \in B(x, R).\ f(y)
  \geq f(x)$.
- $x$ is a local maximum of $f$ if $\exists R > 0.\ \forall y \in B(x, R).\ f(y)
  \leq f(x)$.

Assume that $f$ is differentiable and that $x$ is either a local maximum or a
local minimum. Then $d_xf = 0$. The linear map is one that sends every point to
zero, and all the partial derivatives are zero.

*Proof*:

Let $j \in \{1..n\}$. Define $h: \mathbb{R} \to \mathbb{R}, t \mapsto f(x + t_{e_j})$. Assume
that $x$ s a local minimum of $f$ in $B(x, R)$. Then
$\forall t \in [-R, R), h(t) = f(x + t_{e_j}) \geq f(x)$ and $\norm{x + t_{e_j} -
x} = \norm{t} < R$, so $x + t_{e_j} \in B(x, R)$.

So $x$ is a minimum of $f.h$.

\begin{align*}
h'(0) &= 0 \\
0 = h'(0) &= \lim_{t \to 0} \frac{f(x + t_{e_j}) - f(x)}{t} = \partial_{x_j}f(x)
\end{align*}
Therefore $\nabla f(x) = 0$ and $d_xf(z) = 0 . z = 0 \quad \forall z \in \mathbb{R}^n$.

** TODO Example
\begin{align*}
f: & \mathbb{R}^2 \to \mathbb{R} \\
& (x, y) \mapsto \exp(y^2 x^2)
\end{align*}
$f$ is differentiable since $\partial_xf, \partial_yf$ are continuous as products
and compositions of continuous functions.

This is the differential. The differential is the linear map that gives the best
first-order approximation.
\begin{align*}
d_{(x, y)}f: & \mathbb{R}^2 \to \mathbb{R} \\
& (z_1, z_2) \mapsto \nabla f(x, y) . (z_1, z_2) = exp(y^3 x^2) [2xy^2z_1 + 3y^3x^2z_2]
\end{align*}

* TODO Advanced Linear Algebra
Requires: \nameref{linear-algebra}

We now move to the applications of linear algebra, studying properties of
matrices and vectors in finite-dimensional space.

** TODO Eigenvectors and Eigenvalues

** TODO Diagonalisation

** TODO Jordan Canonical Form

#+BEGIN_LaTeX
\printindex
#+END_LaTeX
